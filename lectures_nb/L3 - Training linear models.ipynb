{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7dd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1a12a",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "In this example we'll work with synthetic data with noise. The target is to get the model that best fits the data.\n",
    "\n",
    "The data is generated using the following function.\n",
    "$$f(x) = 4 + 3 x$$\n",
    "\n",
    "The gaussian noise to the target to simulate the noise in the real dataset. So we have \n",
    "$$y = f(x) + \\mathcal{N}(\\mu=0,\\,\\sigma^{2}=1)$$\n",
    "\n",
    "We'd like to train a model to learn the relation between the input and the output of the data. We can safely assume that the relation is linear, because we generated the data using linear function.\n",
    "The model we're going to use is defined as follows,\n",
    "$$\\hat{y} = h_{\\theta}(x) = \\theta_0 + \\theta_1 * x$$\n",
    "\n",
    "Let's see if we can estimate the original coefficients, i.e., $\\theta_0=4$ and $\\theta_0=3$, that are used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955df6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "np.random.seed(42)\n",
    "\n",
    "ADD_OUTLIERS = False\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "\n",
    "if ADD_OUTLIERS:\n",
    "    y[np.random.randint(100, size=20)] += np.random.randint(-5, 5, size=(20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd5ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points to do prediction on \n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505d8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba92a5ebfe94bfdb5ba3d6e7e65ddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c2160",
   "metadata": {},
   "source": [
    "Last time, we learned about measuring the performance of the regression models. One measure you can use is the $RMSE$. It's easier to use the $MSE$ as a cost function to optimize. Using both will yeild the same result.\n",
    "\n",
    "$$RMSE(X,h)=\\sqrt{\\frac{\\sum_{i=1}^M (h(x^{(i)})-y^{(i)})^2}{M}}$$\n",
    "$$MSE(X,h)=\\frac{\\sum_{i=1}^M (h(x^{(i)})-y^{(i)})^2}{M}$$\n",
    "\n",
    "Another cost function we can use is the Mean Absolute Error $MAE$.\n",
    "$$MAE(X,h)=\\frac{\\sum_{i=1}^M |h(x^{(i)})-y^{(i)}|}{M}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e7d85",
   "metadata": {},
   "source": [
    "### The effect of changing the parameters\n",
    "Let's see how the model and the cost change when we change the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8b6757",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c88fb9eba640edbe9edffbdaf0ad41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45655762b07b4b579b1b76b199f17e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=3.0, description='theta_0', max=10.0), FloatSlider(value=0.0, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 6.5))\n",
    "\n",
    "LIMITS = {\n",
    "    \"theta_0\": [0, 10],\n",
    "    \"theta_1\": [0,  6],\n",
    "    \"cost_l2\"   : [0, 20],\n",
    "    \"cost_l1\"   : [0, 10],\n",
    "    \"cost_log_e\"   : [0, 2],\n",
    "    \"data_x\" : [0,  2],\n",
    "    \"data_y\" : [0, 15]\n",
    "    \n",
    "}\n",
    "\n",
    "axes = {\n",
    "    \"loss_theta0\": fig.add_subplot(2, 2, 1),\n",
    "    \"model\": fig.add_subplot(2, 2, 2),\n",
    "    \"loss_theta1\": fig.add_subplot(2, 2, 3),\n",
    "    \"loss_3d\": fig.add_subplot(2, 2, 4, projection='3d'),\n",
    "}\n",
    "axes[\"model\"].text(0.5, 0.95,r'$h_{\\theta}(x) = \\theta_0+\\theta_1*x$', horizontalalignment='center',\n",
    "     verticalalignment='center',\n",
    "     transform=axes[\"model\"].transAxes)\n",
    "axes[\"model\"].text(0.5, 0.85,r'cost=    ', horizontalalignment='center',\n",
    "         verticalalignment='center',\n",
    "         transform=axes[\"model\"].transAxes)\n",
    "\n",
    "def set_axes_limits(l_func=\"l2\"):\n",
    "    axes[\"model\"].set_xlim(LIMITS[\"data_x\"])\n",
    "    axes[\"model\"].set_ylim(LIMITS[\"data_y\"])\n",
    "    axes[\"model\"].set_xlabel(\"GDP Per Capita\")\n",
    "    axes[\"model\"].set_ylabel(\"Life satisfaction\")\n",
    "    axes[\"loss_theta0\"].set_xlabel(r\"$\\theta_0$\")\n",
    "    axes[\"loss_theta0\"].set_ylabel(\"cost\")\n",
    "    axes[\"loss_theta1\"].set_xlabel(r\"$\\theta_1$\")\n",
    "    axes[\"loss_theta1\"].set_ylabel(\"cost\")\n",
    "    axes[\"loss_3d\"].set_xlabel(r\"$\\theta_0$\")\n",
    "    axes[\"loss_3d\"].set_ylabel(r\"$\\theta_1$\")\n",
    "    axes[\"loss_3d\"].set_zlabel(\"cost\")\n",
    "\n",
    "    if l_func == \"MSE\":\n",
    "        axes[\"loss_theta0\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_theta0\"].set_ylim(LIMITS[\"cost_l2\"])\n",
    "        axes[\"loss_theta1\"].set_xlim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_theta1\"].set_ylim(LIMITS[\"cost_l2\"])\n",
    "        axes[\"loss_3d\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_3d\"].set_ylim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_3d\"].set_zlim(LIMITS[\"cost_l2\"])\n",
    "    elif l_func == \"MAE\":\n",
    "        axes[\"loss_theta0\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_theta0\"].set_ylim(LIMITS[\"cost_l1\"])\n",
    "        axes[\"loss_theta1\"].set_xlim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_theta1\"].set_ylim(LIMITS[\"cost_l1\"])\n",
    "        axes[\"loss_3d\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_3d\"].set_ylim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_3d\"].set_zlim(LIMITS[\"cost_l1\"])\n",
    "    else:\n",
    "        axes[\"loss_theta0\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_theta0\"].set_ylim(LIMITS[\"cost_log_e\"])\n",
    "        axes[\"loss_theta1\"].set_xlim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_theta1\"].set_ylim(LIMITS[\"cost_log_e\"])\n",
    "        axes[\"loss_3d\"].set_xlim(LIMITS[\"theta_0\"])\n",
    "        axes[\"loss_3d\"].set_ylim(LIMITS[\"theta_1\"])\n",
    "        axes[\"loss_3d\"].set_zlim(LIMITS[\"cost_log_e\"])\n",
    "\n",
    "\n",
    "def generic_model(x, theta_0, theta_1):\n",
    "    return theta_0 + x * theta_1\n",
    "\n",
    "def get_2xys_on_model(theta_0, theta_1):\n",
    "    return (LIMITS['data_x'], [generic_model(x, theta_0, theta_1) for x in LIMITS['data_x']])\n",
    "\n",
    "def calc_loss(x, y, theta_0, theta_1, l_func=\"MSE\"):\n",
    "    y_preds = np.array([generic_model(xi, theta_0, theta_1) for xi in x])\n",
    "    if l_func == \"MSE\":\n",
    "        e = ((y_preds - y)**2).mean()\n",
    "    elif l_func == \"MAE\":\n",
    "        e = np.abs((y_preds - y)).mean()\n",
    "    elif l_func == \"MSLE\":\n",
    "        y_preds[y_preds<=0] = 0\n",
    "        e = mean_squared_log_error(y, y_preds)\n",
    "\n",
    "    return e\n",
    "\n",
    "def get_loss_points(theta_0=None, theta_1=None, l_func=\"l2\"):\n",
    "    th0s = np.linspace(*LIMITS['theta_0'], 50)\n",
    "    th1s = np.linspace(*LIMITS['theta_1'], 50)\n",
    "\n",
    "    if theta_0 is None and theta_1 is None:\n",
    "        th0s = np.linspace(*LIMITS['theta_0'], 10)\n",
    "        th1s = np.linspace(*LIMITS['theta_1'], 10)\n",
    "        th0s, th1s = np.meshgrid(th0s, th1s)\n",
    "        mse_loss = np.zeros_like(th0s)\n",
    "        for i in range(len(th0s)):\n",
    "            for j in range(len(th1s)):\n",
    "                mse_loss[i, j] = calc_loss(X, y, th0s[i, j], th1s[i, j], l_func=l_func)\n",
    "        return th0s, th1s, mse_loss\n",
    "    elif theta_1 is None:\n",
    "        xys = [[th1, calc_loss(X, y, theta_0, th1, l_func=l_func)] for th1 in th1s]\n",
    "        return np.array(xys)\n",
    "    elif theta_0 is None:\n",
    "        xys = [[th0, calc_loss(X, y, th0, theta_1, l_func=l_func)] for th0 in th0s]\n",
    "        return np.array(xys, dtype='float')\n",
    "        \n",
    "@widgets.interact(theta_0=(*LIMITS['theta_0'], .1), theta_1=(*LIMITS['theta_1'], .05), loss=[\"MAE\", \"MSE\", \"MSLE\"])\n",
    "def update(theta_0=3, theta_1=0, loss=\"MSE\", show_loss_theta0=False, show_loss_theta1=False, show_loss_3d=False):\n",
    "    \"\"\"Remove old lines from plot and plot new one\"\"\"\n",
    "    set_axes_limits(l_func=loss)\n",
    "    [l.remove() for a in axes.values() for l in a.get_lines()]\n",
    "    [l.remove() for a in axes.values() for l in a.collections]\n",
    "    axes['model'].plot(X, y, 'b.')\n",
    "    axes['model'].plot(*get_2xys_on_model(theta_0, theta_1), 'g')\n",
    "    [tera.remove() for tera in axes[\"model\"].texts[1:]]\n",
    "    axes[\"model\"].text(0.5, 0.85,r'cost={:2.03f}'.format(calc_loss(X, y, theta_0, theta_1, l_func=loss)), horizontalalignment='center',\n",
    "         verticalalignment='center',\n",
    "         transform=axes[\"model\"].transAxes)\n",
    "    if show_loss_theta0:\n",
    "        axes['loss_theta0'].plot(*get_loss_points(theta_1=theta_1, l_func=loss).T, 'b')\n",
    "        axes['loss_theta0'].plot([theta_0], [calc_loss(X, y, theta_0, theta_1, l_func=loss)], 'r.', markersize=12)\n",
    "    if show_loss_theta1:\n",
    "        axes['loss_theta1'].plot(*get_loss_points(theta_0=theta_0, l_func=loss).T, 'b')\n",
    "        axes['loss_theta1'].plot([theta_1], [calc_loss(X, y, theta_0, theta_1, l_func=loss)], 'r.', markersize=12)\n",
    "    if show_loss_3d:\n",
    "        loss_3d_x, loss_3d_y, loss_3d_z = get_loss_points(l_func=loss)\n",
    "        surf = axes['loss_3d'].plot_wireframe(loss_3d_x, loss_3d_y, loss_3d_z, cmap='jet')\n",
    "        surf.set_facecolor((0,0,0,0))\n",
    "        axes['loss_3d'].plot([theta_0], [theta_1], [calc_loss(X, y, theta_0, theta_1, l_func=loss)], 'r.', markersize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87a819",
   "metadata": {},
   "source": [
    "Using the tool, can you guess the parameters values that best describes the data when evaluated by $MSE$?\n",
    "\n",
    "$$\\theta_0=4.20$$\n",
    "$$\\theta_1=2.80$$\n",
    "\n",
    "What if we change the cost function to $MAE$?\n",
    "\n",
    "$$\\theta_0=??$$\n",
    "$$\\theta_1=??$$\n",
    "\n",
    "Can you do the same with `ADD_OUTLIERS`'s value is `True` in the cell we generated the data?\n",
    "\n",
    "MSE\n",
    "$$\\theta_0=??$$\n",
    "$$\\theta_1=??$$\n",
    "\n",
    "MAE\n",
    "$$\\theta_0=??$$\n",
    "$$\\theta_1=??$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8141e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a407dc",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Now that we saw the effect of changing the parameters on the model and the cost function, and manually estimated the values of the parameters, can we use an algorithm to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ad2c",
   "metadata": {},
   "source": [
    "Let's image we have an imaginary cost function $cost(x)=2x^2-3^x+10$. This is a quadratic function which is the type of cost function you'll get when you use $MSE$.\n",
    "\n",
    "How can we get the value of $x$ that minimizes the cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b98146",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_example = \"qu\"\n",
    "\n",
    "cost_qu_func = lambda x: 2*x**2+-3*x+10\n",
    "cost_abs_func = lambda x: abs(-3*x+10) + 2\n",
    "cost_bad_func = lambda x: (-7200*x + 3420*x**2 - (2452*x**3)/3 + (207*x**4)/2 - (33*x**5)/5 + x**6/6)/80 + 78\n",
    "\n",
    "if cost_example == \"qu\":\n",
    "    cost_func = cost_qu_func\n",
    "\n",
    "    LIMITS = {\n",
    "        \"theta\": [-10, 10] ,\n",
    "        \"cost\": [0,  40]    \n",
    "    }\n",
    "elif cost_example == \"abs\":\n",
    "    cost_func = cost_abs_func\n",
    "\n",
    "    LIMITS = {\n",
    "        \"theta\": [-10, 10] ,\n",
    "        \"cost\": [0,  20]    \n",
    "    }\n",
    "elif cost_example == \"bad\":\n",
    "    cost_func = cost_bad_func\n",
    "\n",
    "    LIMITS = {\n",
    "        \"theta\": [1, 12] ,\n",
    "        \"cost\": [0,  10]    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af2c470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f7a1d13a8a4419901c201cc8af843a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c2552cfb442e4abd4e04242ccd5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='x_0', max=12.0, min=-10.0, step=0.05), FloatSlider(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "axes = {\n",
    "    \"cost\": fig.add_subplot(),\n",
    "}\n",
    "\n",
    "axes[\"cost\"].set_xlim(LIMITS[\"theta\"])\n",
    "axes[\"cost\"].set_ylim(LIMITS[\"cost\"])\n",
    "axes[\"cost\"].set_xlabel(r\"$\\theta$\")\n",
    "axes[\"cost\"].set_ylabel(\"cost\")\n",
    "\n",
    "def func_to_optimize(x):\n",
    "    return 0.5*x**2+-3*x+10\n",
    "\n",
    "def get_xys_on_func():\n",
    "    return np.asarray([(x, cost_func(x)) for x in np.linspace(*LIMITS['theta'], 60)])\n",
    "\n",
    "def calc_slope(x, dx):\n",
    "    return (cost_func(x+dx) - cost_func(x))/dx\n",
    "\n",
    "def slope_line_func(x, x_0, dx):\n",
    "    m = calc_slope(x_0, dx)\n",
    "    b = cost_func(x_0) - m * x_0\n",
    "    return m*x+b\n",
    "\n",
    "def get_xys_opt(x_0, lr, steps):\n",
    "    eta = 0.1  # learning rate\n",
    "    n_iterations = 1000\n",
    "    m = 100\n",
    "\n",
    "    theta = np.random.randn(2,1)  # random initialization\n",
    "    x_new = x_0\n",
    "    opt_steps = []\n",
    "    for step in range(steps):\n",
    "        opt_steps.append([x_new, cost_func(x_new)])\n",
    "        gradient = calc_slope(x_new, 1e-5)\n",
    "        x_new = x_new - lr * gradient\n",
    "    return np.asarray(opt_steps)\n",
    "        \n",
    "        \n",
    "@widgets.interact(x_0=(-10,12,0.05), dx=(0.0001, 10, 0.1), lr=(0.001, 1, 0.01), steps=(1, 20, 1))\n",
    "def update(x_0=-1, dx=0.5, lr=0.1, steps=5, show_xdx=False, show_line=False, show_optimization_steps=False):\n",
    "    \"\"\"Remove old lines from plot and plot new one\"\"\"\n",
    "    [l.remove() for a in axes.values() for l in a.get_lines()]\n",
    "    [c.remove() for a in axes.values() for c in a.collections]\n",
    "    [t.remove() for t in axes[\"cost\"].texts]\n",
    "    axes['cost'].plot(*get_xys_on_func().T, 'b')\n",
    "    axes['cost'].plot([x_0], [cost_func(x_0)], 'r.', markersize=12, label=\"$X_0$\")\n",
    "    if show_xdx:\n",
    "        axes['cost'].plot([x_0 + dx], [cost_func(x_0 + dx)], 'g.', markersize=12, label=\"$X_0+\\delta{X}$\")\n",
    "\n",
    "    if show_line:\n",
    "        axes['cost'].plot([-10, 10], [slope_line_func(-10, x_0, dx), slope_line_func(10, x_0, dx)], 'r--', label=\"line\")\n",
    "        axes[\"cost\"].text(0.5, 0.95,r'$slope=\\frac{\\Delta{cost}}{\\Delta{x}}=$' + '{:2.02f}'.format(calc_slope(x_0, dx)), horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform=axes[\"cost\"].transAxes)\n",
    "    if show_optimization_steps:\n",
    "        opt_steps_xys = get_xys_opt(x_0, lr, steps)\n",
    "        for i, xy in enumerate(opt_steps_xys):\n",
    "            axes['cost'].plot([xy[0]], [xy[1]], 'y.', markersize=12, alpha=i/steps, label=\"step{}\".format(i) if steps-5<i else None)\n",
    "    axes['cost'].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f520224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36465b8b",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450ca99",
   "metadata": {},
   "source": [
    "Let's use the knowledge we have about the gradient to guide our steps to optimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e94531",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3158ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09de850",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b14b7f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa29d68d4500455ab11eec9c4266828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ef084",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "479dcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de89390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea39677a4f924628aa54c512d63e1105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:                    # not shown in the book\n",
    "            y_predict = X_new_b.dot(theta)           # not shown\n",
    "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
    "            plt.plot(X_new, y_predict, style)        # not shown\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 # not shown\n",
    "\n",
    "plt.plot(X, y, \"b.\")                                 # not shown\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
    "plt.axis([0, 2, 0, 15])                              # not shown\n",
    "plt.show()                                           # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071d4703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21076011],\n",
       "       [2.74856079]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65f6bb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c2da5cdf504794bcf33c81de9b5e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f781d59f990>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return (t * 0.001 + (1000-t) * 0.1)/1000\n",
    "#     return t0 / (t + t1)\n",
    "\n",
    "ts_temp = np.arange(0, 1000)\n",
    "lr_temp = [learning_schedule(ts) for ts in ts_temp]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts_temp, lr_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc9f25",
   "metadata": {},
   "source": [
    "How can we do that using sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2b6827d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e82d77b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.24365286]), array([2.8250878]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a720d4",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a920f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaeb8a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.25214635],\n",
       "       [2.7896408 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e4fca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5aa036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c28c17a33e486390581fb7e794fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8917c",
   "metadata": {},
   "source": [
    "## Normal equation\n",
    "\n",
    "The linear regression can be solved using the closed-form Normal Equation\n",
    "\n",
    "$$\\hat{\\Theta}=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Where \n",
    "* $X$ is the input for all examples with ones appended to the features for the bias term. $X$ shape is $(m_{examples}, n_{feats}+1)$.\n",
    "* $y$ is all the targets, i.e. $[y^{(1)}, y^{(2)}, ..., y^{(m)}]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22970d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660ff41",
   "metadata": {},
   "source": [
    "How to do the same using sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "619b3a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7eeba",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you could call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae4dab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c2999",
   "metadata": {},
   "source": [
    "This function computes $\\mathbf{X}^+\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "845592be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481493",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342a79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ea8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "X_new = np.linspace(-3, 3, 50).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad8c2b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d823c58f8a407cb29ba101f632fe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49ccb45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "470acc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75275929,  0.56664654])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32ded392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24dc4361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796dfb5e77d447578ca39666345ce502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d953b86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d316c91cfd4a8b82ec01146531f0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbb0e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# m = 20\n",
    "# data_gen_func = lambda X: 1 + 0.5 * X\n",
    "# X = 3 * np.random.rand(m, 1)\n",
    "# y = data_gen_func(X) + np.random.randn(m, 1) / 1.5\n",
    "# X_new = np.linspace(0, 3, 100).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a829ca2",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "What if we add penality to the norm of the parameters?\n",
    "\n",
    "Ridge \n",
    "$$J(\\Theta)=\\frac{\\sum_{i=1}^m (h(x^{(i)})-y^{(i)})^2}{m}+\\alpha\\frac{1}{2}{\\sum_{i=1}^n (\\theta^i)^2}$$\n",
    "\n",
    "LASSO \n",
    "$$J(\\Theta)=\\frac{\\sum_{i=1}^m (h(x^{(i)})-y^{(i)})^2}{m}+\\alpha{\\sum_{i=1}^n |\\theta^i|}$$\n",
    "\n",
    "ElasticNet\n",
    "$$J(\\Theta)=\\frac{\\sum_{i=1}^m (h(x^{(i)})-y^{(i)})^2}{m}+\\frac{1-r}{2}\\alpha{\\sum_{i=1}^n (\\theta^i)^2}+\\frac{r}{2}\\alpha{\\sum_{i=1}^n |\\theta^i|}$$\n",
    "\n",
    "*It's important to notice that we don't regularize the bias term in these equations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "179ec552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756b9d29d6b34b6685b54a4335943e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2bc2acb9634237b641e10160a4d782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_type', options=('Linear', 'Ridge', 'Lasso', 'ElasticNet'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "# plt.figure()\n",
    "# plt.plot(X, y, \"b.\")\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "axes = {\n",
    "    \"model\": fig.add_subplot(),\n",
    "}\n",
    "\n",
    "axes[\"model\"].set_xlim([-3, 3])\n",
    "axes[\"model\"].set_ylim([0, 10])\n",
    "# axes[\"model\"].set_xlim([0, 3])\n",
    "# axes[\"model\"].set_ylim([0, 3])\n",
    "axes[\"model\"].set_xlabel(r\"$x_1$\")\n",
    "axes[\"model\"].set_ylabel(\"$y$\")\n",
    "\n",
    "@widgets.interact(model_type=[\"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\"],\n",
    "                  degree=(1, 100, 1), \n",
    "                  alpha=widgets.FloatLogSlider(value=0, base=10, min=-3, max=3,\n",
    "                                               step=0.2, description='alpha'), \n",
    "                  l1_ratio=(0.00001, 1, 0.1))\n",
    "def update(model_type=\"Linear\", degree=1, alpha=0, l1_ratio=0.5):\n",
    "    [l.remove() for a in axes.values() for l in a.get_lines()]\n",
    "    [c.remove() for a in axes.values() for c in a.collections]\n",
    "    [t.remove() for t in axes[\"model\"].texts]\n",
    "\n",
    "    axes[\"model\"].plot(X, y, \"b.\")\n",
    "#     axes[\"model\"].plot([0, 3], [data_gen_func(0), data_gen_func(1)], \"y--\")\n",
    "    if model_type == \"Linear\" or alpha<1e-5:\n",
    "        model_ins = LinearRegression()\n",
    "        axes[\"model\"].text(0.5, 0.95,r'Model: {}  Degree: {}'.format(model_type, degree), horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform=axes[\"model\"].transAxes)\n",
    "    elif model_type == \"Ridge\":\n",
    "        model_ins = Ridge(alpha=alpha)\n",
    "        axes[\"model\"].text(0.5, 0.95,r'Model: {}  Degree: {}  Alpha: {:.03f}'.format(model_type, degree, alpha), horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform=axes[\"model\"].transAxes)\n",
    "    elif model_type == \"Lasso\":\n",
    "        model_ins = Lasso(alpha=alpha, max_iter=10000)\n",
    "        axes[\"model\"].text(0.5, 0.95,r'Model: {}  Degree: {}  Alpha: {:.03f}'.format(model_type, degree, alpha), horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform=axes[\"model\"].transAxes)\n",
    "    elif model_type == \"ElasticNet\":\n",
    "        model_ins = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=100000)\n",
    "        axes[\"model\"].text(0.5, 0.95,r'Model: {}  Degree: {}  Alpha: {:.03f}  l1_ratio: {:.03f}'.format(model_type, degree, alpha, l1_ratio), horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform=axes[\"model\"].transAxes)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"model\", model_ins),\n",
    "    ])\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_new_regul = model.predict(X_new)\n",
    "    axes[\"model\"].plot(X_new, y_new_regul, \"r\", label=r\"$\\alpha = {}$\".format(alpha))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c3e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab9b4c",
   "metadata": {},
   "source": [
    "### Learning curve to detect overfitting\n",
    "\n",
    "We've learned to visually detetct the overfitting. We also used the validation set to check for overfitting by checking if there's a high difference between the performance measure of the model on the training set and the validation set.\n",
    "\n",
    "We can use another approach to detect the overfitting, the learning curve. In the learning curve, the effect of the dataset size on the performance of the model on the training and the test set is depicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69640353",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77c14729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
    "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3e82326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63db6a9420e4ffe973b50d67ebd95a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
    "plt.show()                                      # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d32fea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff0a8d4a6f545e487ab524a305c9638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "plt.figure()\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           # not shown\n",
    "plt.show()                        # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b514167",
   "metadata": {},
   "source": [
    "#### Bias VS Variance\n",
    "Total error = Bias error + Variance error + Irreducible error\n",
    "\n",
    "* Bias -> wrong assumption Underfitting.\n",
    "* Variance -> Sensitivity to the small variation in the data.\n",
    "* Irreducible error -> Noisiness of the data itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b195ef",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b3d9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7821323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "115888a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e959f285cf3040af8e15c05ebb1da6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "plt.figure()\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "# save_fig(\"early_stopping_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0f5ce",
   "metadata": {},
   "source": [
    "### L1 vs L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "505e70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
    "\n",
    "t1s = np.linspace(t1a, t1b, 500)\n",
    "t2s = np.linspace(t2a, t2b, 500)\n",
    "t1, t2 = np.meshgrid(t1s, t2s)\n",
    "T = np.c_[t1.ravel(), t2.ravel()]\n",
    "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
    "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
    "\n",
    "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
    "\n",
    "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
    "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
    "\n",
    "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
    "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
    "\n",
    "t_init = np.array([[0.25], [-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01fb100a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc50e55ee284d2fbc9f03a6faf1eeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200):\n",
    "    path = [theta]\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta\n",
    "        theta = theta - eta * gradients\n",
    "        path.append(theta)\n",
    "    return np.array(path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
    "for i, N, l1, l2, title in ((0, N1, 2., 0, \"Lasso\"), (1, N2, 0,  2., \"Ridge\")):\n",
    "    JR = J + l1 * N1 + l2 * 0.5 * N2**2\n",
    "    \n",
    "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
    "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
    "\n",
    "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
    "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
    "    levelsN=np.linspace(0, np.max(N), 10)\n",
    "    \n",
    "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
    "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
    "    path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
    "\n",
    "    ax = axes[i, 0]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, N / 2., levels=levelsN)\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.set_title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\theta_2$\", fontsize=16, rotation=0)\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
    "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b54e7",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "We can do classification with a small tweak in the linear regression model.\n",
    "$$\\hat{p}=h_{\\Theta}(x)=\\sigma(x^T\\Theta)$$\n",
    "Where $\\sigma()$ is the logistic function,\n",
    "$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a27643e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399dc4b08b334f37b7cf9b4f814e87ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8c8ea",
   "metadata": {},
   "source": [
    "One loss functions that can be used for logistic regression is the *log loss*\n",
    "$$J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m [y^{(i)}\\log(\\hat{p}^{(i)})+(1-y^{(i)})\\log(1-\\hat{p}^{(i)})]$$\n",
    "\n",
    "Partial derivatives\n",
    "$$\\frac{\\partial {J(\\theta)}}{\\partial {\\theta_j}}=\\frac{1}{m}\\sum_{i=1}^m (\\sigma(\\theta^Tx^{(i)}) - y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cb68288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c496ae4354c4e1f828f476c70b2e0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f78157c5c10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_log = np.linspace(0.001, 0.999, 30)\n",
    "y_log = -np.log(X_log)\n",
    "plt.figure()\n",
    "plt.plot(X_log, y_log, label=\"positive examples\")\n",
    "y_log = -np.log(1 - X_log)\n",
    "plt.plot(X_log, y_log, label=\"negative examples\")\n",
    "plt.xlabel(\"$\\hat{p}$\")\n",
    "plt.ylabel(\"$-\\log{(p)}$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b42f2cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b32a956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d782323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72e5b6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140519e756934f9192c4994d600c02e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f781552d750>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9768a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e6257cdc034f649cce5a2977f0de6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elawady/miniconda3/envs/tf2/lib/python3.7/site-packages/matplotlib/patches.py:1338: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  verts = np.dot(coords, M) + (x + dx, y + dy)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7f899ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.66066066])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "edc5edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88f5c177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4cda92a9834b62908b73b94203f44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a6e9a",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Score for each class\n",
    "$$s_k(\\boldsymbol{x})=\\boldsymbol{x}^T\\Theta^{(k)}$$\n",
    "\n",
    "The output\n",
    "$$\\hat{p}_k=\\frac{exp(s_k(\\boldsymbol{x}))}{\\sum_{j=1}^K exp(s_j(\\boldsymbol{x}))}$$\n",
    "\n",
    "The loss function\n",
    "$$J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^Ky_k^{(i)}\\log(\\hat{p}_k^{(i)})$$\n",
    "\n",
    "Partial derivatives\n",
    "$$\\nabla_{\\Theta^{(k)}}{J(\\Theta)}=\\frac{1}{m}\\sum_{i=1}^m (\\hat{p}_k^{(i)} - y_k^{(i)})x^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a9735b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial', random_state=42)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8f78c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d451752b584aa184bbaaf5df245d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eec05c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ec4492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5, 2]]).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
